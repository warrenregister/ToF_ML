{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN with Fragment and No Mans Land.ipynb","provenance":[],"mount_file_id":"1ckrijxaAue0dtJ4gTCauRIXcRWK7h4IW","authorship_tag":"ABX9TyMR5qZ/PCDRBr1vr3neUhbN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"O1F7u8XZRaqa","executionInfo":{"status":"ok","timestamp":1617918831993,"user_tz":420,"elapsed":2430,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWdkkgpILKNC","executionInfo":{"status":"ok","timestamp":1617918831994,"user_tz":420,"elapsed":2314,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}}},"source":["%load_ext tensorboard\n","import datetime, os\n","log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERkPqNQuTMAB","executionInfo":{"status":"ok","timestamp":1617918832254,"user_tz":420,"elapsed":2569,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}},"outputId":"a231949c-4520-4963-8e0a-1fa116e97470"},"source":["%cd /content/drive/MyDrive/PHI/ToF_ML/src"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/PHI/ToF_ML/src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o12YZCo-TMCO","executionInfo":{"status":"ok","timestamp":1617918836592,"user_tz":420,"elapsed":6905,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}}},"source":["from ast import literal_eval\n","data = pd.read_csv('../data/fixed_1400.csv')\n","data['masses'] = data['masses'].apply(literal_eval)\n","data['channels'] = data['channels'].apply(literal_eval)\n","data['intensities'] = data['intensities'].apply(literal_eval)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOVytZ-nTMEq","colab":{"base_uri":"https://localhost:8080/","height":530},"executionInfo":{"status":"ok","timestamp":1617918836884,"user_tz":420,"elapsed":7191,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}},"outputId":"bf7f71af-e1b2-4bb2-9b6a-6a4ea6cf2685"},"source":["data.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>Mass/Time</th>\n","      <th>MassOffset</th>\n","      <th>StartFlightTime</th>\n","      <th>SpecBinSize</th>\n","      <th>channels</th>\n","      <th>intensities</th>\n","      <th>masses</th>\n","      <th>avg_dist_frags_low</th>\n","      <th>avg_dist_frags_high</th>\n","      <th>adjusted_original_proportions_identified</th>\n","      <th>original_proportions_identified</th>\n","      <th>diff</th>\n","      <th>prop_diff_in_low</th>\n","      <th>calibration</th>\n","      <th>adjusted_proportion_identified</th>\n","      <th>proportion_identified</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0106301.cas</td>\n","      <td>0.387384</td>\n","      <td>-0.275004</td>\n","      <td>0.0</td>\n","      <td>1.248</td>\n","      <td>[2644.0367300000003, 3505.0183700000002, 4162....</td>\n","      <td>[73874, 1234, 138, 610, 1216, 4159, 8958, 1084...</td>\n","      <td>[1.0065519723918102, 2.015029094672708, 3.0191...</td>\n","      <td>0.001298</td>\n","      <td>0.002255</td>\n","      <td>0.418033</td>\n","      <td>0.398438</td>\n","      <td>0.000958</td>\n","      <td>0.738174</td>\n","      <td>0</td>\n","      <td>0.540984</td>\n","      <td>0.515625</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0107316.cas</td>\n","      <td>0.387113</td>\n","      <td>-0.278302</td>\n","      <td>0.0</td>\n","      <td>1.248</td>\n","      <td>[2647.00072, 3508.9949100000003, 4164.59326000...</td>\n","      <td>[49864, 1034, 168, 4696, 8247, 13992, 17903, 2...</td>\n","      <td>[1.00101811517532, 2.0077555328930656, 3.00565...</td>\n","      <td>0.001537</td>\n","      <td>0.002586</td>\n","      <td>0.131783</td>\n","      <td>0.129771</td>\n","      <td>0.001049</td>\n","      <td>0.682225</td>\n","      <td>0</td>\n","      <td>0.519380</td>\n","      <td>0.511450</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0110203.cas</td>\n","      <td>0.379037</td>\n","      <td>-0.271056</td>\n","      <td>0.0</td>\n","      <td>4.992</td>\n","      <td>[1973.87665, 2049.0706800000003, 2122.01224, 2...</td>\n","      <td>[23352, 74717, 10387, 947, 12344, 9121, 249, 4...</td>\n","      <td>[11.998071176139083, 13.003971096434277, 14.01...</td>\n","      <td>0.001640</td>\n","      <td>0.001858</td>\n","      <td>0.388889</td>\n","      <td>0.388889</td>\n","      <td>0.000218</td>\n","      <td>0.133100</td>\n","      <td>0</td>\n","      <td>0.444444</td>\n","      <td>0.444444</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0110212.cas</td>\n","      <td>0.379177</td>\n","      <td>-0.269744</td>\n","      <td>0.0</td>\n","      <td>4.992</td>\n","      <td>[672.00298, 891.90543, 1970.94521, 2046.11295,...</td>\n","      <td>[34398, 304, 2223, 3521, 5205, 6509, 99, 115, ...</td>\n","      <td>[1.0045194511091773, 2.012140966655108, 11.978...</td>\n","      <td>0.001337</td>\n","      <td>0.002206</td>\n","      <td>0.379630</td>\n","      <td>0.379630</td>\n","      <td>0.000868</td>\n","      <td>0.649178</td>\n","      <td>0</td>\n","      <td>0.592593</td>\n","      <td>0.592593</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0116511.cas</td>\n","      <td>0.383360</td>\n","      <td>-0.302184</td>\n","      <td>0.0</td>\n","      <td>1.248</td>\n","      <td>[2726.98153, 3594.53649, 4265.28736, 7866.5038...</td>\n","      <td>[42995, 602, 151, 17912, 9536, 17609, 29604, 5...</td>\n","      <td>[1.0049940659400325, 2.0094784706009245, 3.022...</td>\n","      <td>0.001397</td>\n","      <td>0.002171</td>\n","      <td>0.418367</td>\n","      <td>0.336066</td>\n","      <td>0.000774</td>\n","      <td>0.554114</td>\n","      <td>0</td>\n","      <td>0.581633</td>\n","      <td>0.467213</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     file_name  ...  proportion_identified\n","0  0106301.cas  ...               0.515625\n","1  0107316.cas  ...               0.511450\n","2  0110203.cas  ...               0.444444\n","3  0110212.cas  ...               0.592593\n","4  0116511.cas  ...               0.467213\n","\n","[5 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"G23PQWctTMHF","executionInfo":{"status":"ok","timestamp":1617918840621,"user_tz":420,"elapsed":10925,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}}},"source":["from data_transformation import generate_data, mass_formula, generate_calibrated_data\n","erred = generate_data(data, 2, 2, [0, 0, 0], slope_index=2)\n","for num in range(3):\n","    for _ in range(10):\n","        erred = pd.concat([erred, generate_data(data, num + 2, 1, [0.334, 0.667, 1], True, True, slope_index=2)], axis=0)\n","#erred['target'] = erred['target'].apply(lambda a: a - 1 if a > 0 else a)\n","erred.reset_index(inplace=True, drop=True)\n","erred = generate_calibrated_data(erred, slope_index=2)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWJitbrkTMLi","executionInfo":{"status":"ok","timestamp":1617918840623,"user_tz":420,"elapsed":10926,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}}},"source":["def get_spectra_summary(masses, scaled_intens, slope, offset, length=2000):\n","    # scaled intens might not be a good tie breaker for peaks at same mass\n","    summary = np.zeros([2, length + 1])\n","    i = 0\n","    while 1:\n","        mass = masses[i]\n","        if mass < length:\n","            intensity = scaled_intens[i]\n","            index = round(mass)\n","            if summary[1][index] == 0:\n","                summary[1][index] = mass - round(mass)\n","                summary[0][index] = intensity\n","            else:\n","                if intensity > summary[0][index]:\n","                    summary[1][index] = mass - round(mass)\n","                    summary[0][index] = intensity\n","        i += 1\n","        if i >= len(masses):\n","            break\n","    summary[0][-1] += slope\n","    summary[1][-1] += offset\n","    return summary"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"b9dHBH_3fBD3"},"source":["from sklearn.preprocessing import MinMaxScaler\n","scl = MinMaxScaler()\n","scaled_intens = []\n","for row in erred.itertuples():\n","    scl.fit(np.array(row.intensities).reshape((-1, 1)))\n","    intensities = scl.transform(np.array(row.intensities).reshape((-1, 1)))\n","    scaled_intens.append(intensities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHRMB9IcDqgy"},"source":["errors = np.array(erred[['err_prop_slope', 'err_prop_offset']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1DXaC9vWFsE"},"source":["X = np.zeros((len(erred), 2, 2001))\n","y = np.array(pd.get_dummies(erred['target']))\n","y = np.concatenate([y, errors], axis=1)\n","for i, row in enumerate(erred.itertuples()):\n","    summary = get_spectra_summary(row.masses, scaled_intens[i], row[2],\n","                                  row.MassOffset)\n","    X[i] += summary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_cQQ8qzgO-C"},"source":["from sklearn.model_selection import train_test_split\n","indices = np.concatenate((np.random.randint(0, 1441, 10), np.random.randint(1441, X.shape[0], 270)))\n","X_val = X[indices]\n","y_val = y[indices]\n","X = np.delete(X, indices, axis=0)\n","y = np.delete(y, indices, axis=0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztXjGitwhY3L"},"source":["X_train = tf.convert_to_tensor(X_train)\n","X_test = tf.convert_to_tensor(X_test)\n","X_val = tf.convert_to_tensor(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eexY7psrdIJE"},"source":["def split_y(y):\n","    dummy = y.copy()\n","    split = np.hsplit(dummy, np.array([3, 6]))\n","    dummy = {'error_cat': split[0], 'error_amt': split[1]}\n","    return dummy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4lNWlm6efD1"},"source":["y_train = split_y(y_train)\n","y_test = split_y(y_test)\n","y_val = split_y(y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7dTMbQzTMQl","executionInfo":{"status":"ok","timestamp":1617912583946,"user_tz":420,"elapsed":103070,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}},"outputId":"f99b494f-0cd5-4432-e12f-9e10e1fcd437"},"source":["if tf.test.gpu_device_name():\n","    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n","else:\n","    print(\"Please install GPU version of TF\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Default GPU Device:/device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E7bmP_1MBxbD"},"source":["class_weights = {0: 1 / (np.sum(erred['target']==0) / len(erred)), 1: 1/ (np.sum(erred['target']==1) / len(erred)), 2: 1 / (np.sum(erred['target']==2) / len(erred))}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tm0fz8tjTapc"},"source":["import tensorflow.keras as keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzfgoGBPHcaf"},"source":["def weighted_categorical_crossentropy(y_true, y_pred, weights):\n","    nb_cl = len(weights)\n","    final_mask = K.zeros_like(y_pred[:, 0])\n","    y_pred_max = K.max(y_pred, axis=1)\n","    y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n","    y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())\n","    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n","        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n","    return K.categorical_crossentropy(y_pred, y_true) * final_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciWZR5hWTMS8"},"source":["def calculate_metrics(y_true, y_pred, duration):\n","    res = pd.DataFrame(data=np.zeros((1, 4), dtype=np.float), index=[0],\n","                       columns=['precision', 'accuracy', 'recall', 'duration'])\n","    res['precision'] = precision_score(y_true, y_pred, average='macro')\n","    res['accuracy'] = accuracy_score(y_true, y_pred)\n","    res['recall'] = recall_score(y_true, y_pred, average='macro')\n","    res['duration'] = duration\n","    return res\n","\n","\n","def save_test_duration(file_name, test_duration):\n","    res = pd.DataFrame(data=np.zeros((1, 1), dtype=np.float), index=[0],\n","                       columns=['test_duration'])\n","    res['test_duration'] = test_duration\n","    res.to_csv(file_name, index=False)\n","\n","\n","def save_logs(output_directory, hist, y_pred, y_true, duration,\n","              lr=True, plot_test_acc=True):\n","    hist_df = pd.DataFrame(hist.history)\n","    hist_df.to_csv(output_directory + 'history.csv', index=False)\n","\n","    df_metrics = calculate_metrics(y_true, y_pred, duration)\n","    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n","\n","    index_best_model = hist_df['loss'].idxmin()\n","    row_best_model = hist_df.loc[index_best_model]\n","\n","    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float),\n","                                 index=[0],\n","                                 columns=['best_model_train_loss',\n","                                          'best_model_val_loss',\n","                                          'best_model_train_acc',\n","                                          'best_model_val_acc',\n","                                          'best_model_learning_rate',\n","                                          'best_model_nb_epoch'])\n","    df_best_model['best_model_train_loss'] = row_best_model['loss']\n","    if plot_test_acc:\n","        df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n","    df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n","    if plot_test_acc:\n","        df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n","    if lr == True:\n","        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n","    df_best_model['best_model_nb_epoch'] = index_best_model\n","\n","    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n","\n","    if plot_test_acc:\n","        # plot losses\n","        plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n","\n","    return df_metrics\n","\n","\n","def plot_epochs_metric(hist, file_name, metric='loss'):\n","    plt.figure()\n","    plt.plot(hist.history[metric])\n","    plt.plot(hist.history['val_' + metric])\n","    plt.title('model ' + metric)\n","    plt.ylabel(metric, fontsize='large')\n","    plt.xlabel('epoch', fontsize='large')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.savefig(file_name, bbox_inches='tight')\n","    plt.close()\n","\n","\n","class Classifier_INCEPTION:\n","\n","    def __init__(self, output_directory, input_shape, nb_classes, verbose=False,\n","                 build=True, batch_size=64, nb_filters=32, use_residual=True,\n","                 use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500):\n","\n","        self.output_directory = output_directory\n","\n","        self.nb_filters = nb_filters\n","        self.use_residual = use_residual\n","        self.use_bottleneck = use_bottleneck\n","        self.depth = depth\n","        self.kernel_size = kernel_size - 1\n","        self.callbacks = None\n","        self.batch_size = batch_size\n","        self.bottleneck_size = 32\n","        self.nb_epochs = nb_epochs\n","        self.verbose = verbose\n","\n","        if build == True:\n","            self.model = self.build_model(input_shape, nb_classes)\n","            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n","\n","    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n","\n","        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n","            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n","                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n","        else:\n","            input_inception = input_tensor\n","\n","        # kernel_size_s = [3, 5, 8, 11, 17]\n","        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n","\n","        conv_list = []\n","\n","        for i in range(len(kernel_size_s)):\n","            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n","                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n","                input_inception))\n","\n","        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n","\n","        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n","                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n","\n","        conv_list.append(conv_6)\n","\n","        x = keras.layers.Concatenate(axis=2)(conv_list)\n","        x = keras.layers.BatchNormalization()(x)\n","        x = keras.layers.Activation(activation='relu')(x)\n","        return x\n","\n","    def _shortcut_layer(self, input_tensor, out_tensor):\n","        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n","                                         padding='same', use_bias=False)(input_tensor)\n","        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n","\n","        x = keras.layers.Add()([shortcut_y, out_tensor])\n","        x = keras.layers.Activation('relu')(x)\n","        return x\n","\n","    def build_model(self, input_shape, nb_classes):\n","        input_layer = keras.layers.Input(input_shape)\n","        calib_values = keras.layers.Lambda(lambda x: x[:, :, -1])(input_layer)\n","        spectrum = keras.layers.Lambda(lambda x: x[:, :, :-1])(input_layer)\n","\n","        input_dropout = keras.layers.Dropout(rate=0.2)(spectrum)\n","        x = input_dropout\n","        input_res = input_dropout\n","\n","        for d in range(self.depth):\n","\n","            x = self._inception_module(x)\n","\n","            if self.use_residual and d % 3 == 2:\n","                x = self._shortcut_layer(input_res, x)\n","                input_res = x\n","\n","        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n","\n","        concat = keras.layers.Concatenate()([gap_layer, calib_values])\n","\n","        dropout_gap = keras.layers.Dropout(rate=.2)(gap_layer)\n","\n","        dl = keras.layers.Dense(128)(dropout_gap)\n","\n","        dropout_dense = keras.layers.Dropout(rate=.2)(dl)\n","\n","        dl2 = keras.layers.Dense(256)(dropout_dense)\n","\n","        error_output_layer = keras.layers.Dense(2, activation='linear')(dl2)\n","\n","        concat_final = keras.layers.Concatenate()([dl2, error_output_layer])\n","\n","        class_output_layer = keras.layers.Dense(nb_classes,\n","                                          activation='softmax')(concat_final)\n","\n","        model = keras.models.Model(inputs=input_layer,\n","                                   outputs={'error_cat':class_output_layer,\n","                                            'error_amt': error_output_layer})\n","\n","        model.compile(loss={'error_cat':'categorical_crossentropy',\n","                            'error_amt': 'mse'},\n","                      loss_weights = {'error_cat': .3, \n","                                      'error_amt': .6},\n","                      optimizer=keras.optimizers.Adam(),\n","                      metrics={'error_cat': ['accuracy', 'AUC'],\n","                               'error_amt': ['mse']})\n","\n","        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss',\n","                                                      factor=0.5, patience=50,\n","                                                      min_lr=0.0001)\n","\n","        file_path = self.output_directory + 'best_model.hdf5'\n","\n","        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path,\n","                                                           monitor='loss',\n","                                                           save_best_only=True,)\n","        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir,\n","                                                              histogram_freq=1)\n","        \n","        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                                       patience=20)\n","\n","\n","        self.callbacks = [reduce_lr, model_checkpoint, tensorboard_callback,\n","                          early_stopping]\n","\n","        return model\n","\n","    def fit(self, x_train, y_train, x_val, y_val, y_true, plot_test_acc=False):\n","        if not tf.test.gpu_device_name():\n","            print('error no gpu')\n","            exit()\n","        # x_val and y_val are only used to monitor the test loss and NOT for training\n","\n","        if self.batch_size is None:\n","            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n","        else:\n","            mini_batch_size = self.batch_size\n","\n","        start_time = time.time()\n","\n","        if plot_test_acc:\n","\n","            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size,\n","                                  epochs=self.nb_epochs, verbose=self.verbose,\n","                                  validation_data=(x_val, y_val),\n","                                  callbacks=self.callbacks)\n","        else:\n","\n","            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size,\n","                                  epochs=self.nb_epochs, verbose=self.verbose,\n","                                  callbacks=self.callbacks)\n","\n","        duration = time.time() - start_time\n","\n","        self.model.save(self.output_directory + 'last_model.hdf5')\n","\n","        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n","                              return_df_metrics=False)\n","\n","        # save predictions\n","        np.save(self.output_directory + 'y_pred.npy', y_pred)\n","\n","        # convert the predicted from binary to integer\n","        y_pred = np.argmax(y_pred, axis=1)\n","\n","        df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration,\n","                             plot_test_acc=plot_test_acc)\n","\n","        keras.backend.clear_session()\n","\n","        return 0#\n","\n","    def predict(self, x_test, y_true, x_train, y_train, y_test,\n","                return_df_metrics=True):\n","        start_time = time.time()\n","        model_path = self.output_directory + 'best_model.hdf5'\n","        model = keras.models.load_model(model_path)\n","        y_pred = model.predict(x_test, batch_size=self.batch_size)\n","        if return_df_metrics:\n","            y_pred = np.argmax(y_pred, axis=1)\n","            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n","            return df_metrics\n","        else:\n","            test_duration = time.time() - start_time\n","            save_test_duration(self.output_directory + 'test_duration.csv',\n","                               test_duration)\n","            return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oX_OvyhjTfgT","executionInfo":{"status":"ok","timestamp":1617912925041,"user_tz":420,"elapsed":476,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}},"outputId":"2c421d33-330f-4140-dc66-bb5c08866920"},"source":["X_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([35514, 2, 2001])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"HMr1fEZfiUGl"},"source":["import time\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.preprocessing import LabelEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u1ZELwyah37j"},"source":["c = Classifier_INCEPTION('../data/', (2, 2001), 3, build=True, verbose=True, batch_size=60, nb_epochs=600, depth=5, use_bottleneck=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqYnZB2Zkepa"},"source":["c.model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5uePJeRiXWc"},"source":["c.fit(X_train, y_train, X_val, y_val, y_val, plot_test_acc=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FmrsRDkxNhU"},"source":["c.model.save('../models/scaled_intens_whole_num_subbed_masses_3_cat_dropout')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hykJW09ux1La","executionInfo":{"status":"ok","timestamp":1617756130508,"user_tz":420,"elapsed":5061,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}},"outputId":"27e43ccc-e976-4d78-adb2-754ba1354b3c"},"source":[" keras.models.load_model('../models/scaled_intens_whole_num_subbed_masses')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.engine.functional.Functional at 0x7fabe3e13150>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":134},"id":"lUPqrEb8_HVi","executionInfo":{"status":"ok","timestamp":1617826911332,"user_tz":420,"elapsed":1572,"user":{"displayName":"Warren Register","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN7Wja7DSoALRpu3eATQH0nUFvY5rFY5yaljwIbw=s64","userId":"14502775680021541472"}},"outputId":"a83fbeea-4d30-4f0c-f68c-041eac6c75e2"},"source":["c.predict(X_val, y_val, X_train, y_train, y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>precision</th>\n","      <th>accuracy</th>\n","      <th>recall</th>\n","      <th>duration</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.500845</td>\n","      <td>0.792857</td>\n","      <td>0.544176</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   precision  accuracy    recall  duration\n","0   0.500845  0.792857  0.544176       0.0"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"vZMAjaFnTfj2"},"source":["%tensorboard --logdir log_dir"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CxWO_yG2ACnU"},"source":["from keras.utils import to_categorical\n","from PIL import Image\n","# Data Generator Example: read in data small amt at a time to save memory\n","# https://towardsdatascience.com/building-a-multi-output-convolutional-neural-network-with-keras-ed24c7bc1178\n","class UtkFaceDataGenerator():\n","    \"\"\"\n","    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\n","    \"\"\"\n","    def __init__(self, df):\n","        self.df = df\n","        \n","    def generate_split_indexes(self):\n","        p = np.random.permutation(len(self.df))\n","        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\n","        train_idx = p[:train_up_to]\n","        test_idx = p[train_up_to:]\n","        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n","        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n","        \n","        # converts alias to id\n","        self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\n","        self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\n","        self.max_age = self.df['age'].max()\n","        \n","        return train_idx, valid_idx, test_idx\n","    \n","    def preprocess_image(self, img_path):\n","        \"\"\"\n","        Used to perform some minor preprocessing on the image before inputting into the network.\n","        \"\"\"\n","        im = Image.open(img_path)\n","        im = im.resize((IM_WIDTH, IM_HEIGHT))\n","        im = np.array(im) / 255.0\n","        \n","        return im\n","        \n","    def generate_images(self, image_idx, is_training, batch_size=16):\n","        \"\"\"\n","        Used to generate a batch with images when training/testing/validating our Keras model.\n","        \"\"\"\n","        \n","        # arrays to store our batched data\n","        images, ages, races, genders = [], [], [], []\n","        while True:\n","            for idx in image_idx:\n","                person = self.df.iloc[idx]\n","                \n","                age = person['age']\n","                race = person['race_id']\n","                gender = person['gender_id']\n","                file = person['file']\n","                \n","                im = self.preprocess_image(file)\n","                \n","                ages.append(age / self.max_age)\n","                races.append(to_categorical(race, len(dataset_dict['race_id'])))\n","                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\n","                images.append(im)\n","                \n","                # yielding condition\n","                if len(images) >= batch_size:\n","                    yield np.array(images), [np.array(ages), np.array(races), np.array(genders)]\n","                    images, ages, races, genders = [], [], [], []\n","                    \n","            if not is_training:\n","                break\n","                \n","data_generator = UtkFaceDataGenerator(df)\n","train_idx, valid_idx, test_idx = data_generator.generate_split_indexes() "],"execution_count":null,"outputs":[]}]}