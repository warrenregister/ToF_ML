{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sys import path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.insert(0, '../src')\n",
    "from setup import data_setup\n",
    "from data_generator import DataGenerator\n",
    "from model_trainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = data_setup()\n",
    "norm_data = dg.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_transformation import get_isotope_data, get_hydrocarbs\n",
    "isotope_data = get_isotope_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_transformation import get_suspicious_peaks, get_peak_suspiciousness, get_ranges\n",
    "original_data = dg.calibrated_df(True, .005, .01,use_ranges=True, ranges=[0, 0, 0.5], cat=True)\n",
    "ranges = get_ranges(isotope_data, 2000)\n",
    "original_data['target'] = original_data['target'].apply(lambda a: a - 1 if a > 0 else a)\n",
    "original_data['sus_peaks'] = original_data['masses'].apply(get_suspicious_peaks, args=(ranges, .1))\n",
    "original_data['peak_sussness'] = original_data['masses'].apply(get_peak_suspiciousness, args=(ranges, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_transformation import mass_formula\n",
    "diffs = []\n",
    "errs = []\n",
    "no_mans_lands_err = []\n",
    "no_mans_lands_norm = []\n",
    "masses = []\n",
    "means = 0\n",
    "for row in norm_data.itertuples():\n",
    "    diff = []\n",
    "    mass = []\n",
    "    nml_err = []\n",
    "    nml_norm = []\n",
    "    mean = 0\n",
    "    err_offset, amt_err_offset = add_error(row.MassOffset)\n",
    "    errs.append(amt_err_offset)\n",
    "    for peak in row.peaks:\n",
    "        init_mass = mass_formula(peak[0], row.SpecBinSize, row.StartFlightTime, row[4], row.MassOffset)\n",
    "        i = int(init_mass)\n",
    "        if init_mass > ranges[i][0] and init_mass < ranges[i][1]:\n",
    "            nml_norm.append(init_mass)                                                \n",
    "        new_mass = mass_formula(peak[0], row.SpecBinSize, row.StartFlightTime, row[4], err_offset)\n",
    "        i = int(new_mass)\n",
    "        if new_mass > ranges[i][0] and new_mass < ranges[i][1]:\n",
    "            nml_err.append(new_mass)  \n",
    "        mass.append(init_mass)\n",
    "        diff.append(init_mass - new_mass)\n",
    "        mean += abs(init_mass - new_mass)\n",
    "    diffs.append(diff)\n",
    "    no_mans_lands_err.append(nml_err)\n",
    "    no_mans_lands_norm.append(nml_norm)\n",
    "    masses.append(mass)\n",
    "    mean = mean / len(row.peaks)\n",
    "    means += mean\n",
    "means = means / len(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_cas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df['Calibration'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "positions = []\n",
    "for row in df.itertuples():\n",
    "    for split in row.Calibration.split(')')[:-1]:\n",
    "        calibrators = split.split('(')[1].split(',')\n",
    "        if calibrators[1] not in labels:\n",
    "            labels.append(calibrators[1])\n",
    "            positions.append(float(calibrators[2].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'labels': labels, 'positions': positions})\n",
    "df2 = df2.sort_values('positions', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(masses, intensities, x=12, thresh=0.1):\n",
    "    '''\n",
    "    Get all peaks in data near a specific mass x.\n",
    "    '''\n",
    "    row_x = -1\n",
    "    max = -1\n",
    "    for i, mass in enumerate(masses):\n",
    "        dif = abs(mass-x)\n",
    "        inten = intensities[i]\n",
    "        if dif < thresh and (inten > max or max == -1):\n",
    "            max = inten\n",
    "            row_x = dif\n",
    "    return row_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame(columns=list(df2['labels'])+['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_transformation import generate_data\n",
    "# 10,000 Examples only offset error\n",
    "erred = generate_data(norm_data, 2, 2, True, [0, 0, 0])\n",
    "for _ in range(10):\n",
    "    erred = pd.concat([erred, generate_data(norm_data, 2, 2, True, [0, 0, 1], True)], axis=0)\n",
    "#erred['target'] = erred['target'].apply(lambda a: a - 1 if a > 0 else a)\n",
    "dg.set_df(erred)\n",
    "erred = dg.calibrated_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(erred.itertuples()):\n",
    "    dists = []\n",
    "    for pos in df2['positions']:\n",
    "        dists.append(get_x(row.masses, row.intensities, x=pos, thresh=0.1))\n",
    "    training_data.loc[i] = dists + [row.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = training_data.drop('target', axis=1)\n",
    "X = X.to_numpy().reshape(9966, 309, 1)\n",
    "y = training_data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Input, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling1D, MaxPooling1D, Layer, Concatenate\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=8, kernel_size=10, activation='tanh', input_shape=(309, 1)))\n",
    "    model.add(AveragePooling1D())\n",
    "\n",
    "    model.add(Conv1D(filters=16, kernel_size=20, activation='tanh'))\n",
    "    model.add(AveragePooling1D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=500, activation='tanh'))\n",
    "\n",
    "    model.add(Dense(units=100, activation='tanh'))\n",
    "\n",
    "    model.add(Dense(units=2, activation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator2 = KerasClassifier(build_fn = lenet, epochs = 200, batch_size = 20, verbose = 1, class_weight={0:11, 1:1})\n",
    "estimator2.fit(X_train, y_train)\n",
    "preds2 = estimator2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "preds2 = estimator2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6692    1.0\n",
       "7286    1.0\n",
       "1111    1.0\n",
       "1672    1.0\n",
       "8247    1.0\n",
       "       ... \n",
       "8604    1.0\n",
       "1287    1.0\n",
       "7592    1.0\n",
       "8959    1.0\n",
       "8244    1.0\n",
       "Name: target, Length: 1994, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
